{"cells":[{"cell_type":"code","execution_count":2,"id":"61d1211d","metadata":{"scrolled":true,"id":"61d1211d","executionInfo":{"status":"ok","timestamp":1701400163247,"user_tz":360,"elapsed":84,"user":{"displayName":"Dina Albassam","userId":"16100326760642947534"}}},"outputs":[],"source":["import os\n","import csv\n","import spacy\n","import torch\n","import numpy as np\n","import networkx as nx\n","from tqdm import tqdm\n","from collections import Counter\n","from nltk import sent_tokenize, word_tokenize\n","from transformers import pipeline, T5Tokenizer, T5Config, T5ForConditionalGeneration\n","\n","\n","\n","def load_data(path):\n","    # Load multiple documents from the path.\n","    # Each document describes one event instance.\n","    data = []\n","    trigger = []\n","    file_names = os.listdir('RoleEE_data/Clinical_Note')\n","    print(file_names)\n","    for name in file_names:\n","        print(name)\n","        if '.txt' in name:\n","            f = open(path + name, 'r')\n","            lines = f.readlines()\n","            text = ' '.join(lines)\n","            data.append(text)\n","            # The name of each document is regarded as the event trigger.\n","            # Note that the triggers are only used to identify different events. They won't be used for role prediction.\n","            trigger.append(name[:-4])\n","            print(data)\n","    return data, trigger\n","\n","\n","\n","def truncate_text(text, max_length=350):\n","    # Truncate each document for subsequent processing\n","    # Note that the length of the truncated text here should be less than the maximum length that pretrained models can process.\n","    # Because the prompt will be added to generate candidate roles\n","    truncated = []\n","    for doc in text:\n","        string = []\n","        cnt_w = 0\n","        sents = sent_tokenize(doc)\n","        for s in sents:\n","            c = len(word_tokenize(s))\n","            if cnt_w + c > max_length:\n","                break\n","            cnt_w += c\n","            string.append(s)\n","        truncated.append(string)\n","    return truncated\n","\n","\n","def recognize_named_entity(text, nlp):\n","    # Recognize all named entities and their types using the spacy NER tool\n","    def cleanup(token, lower = True):\n","        if lower:\n","            token = token.lower()\n","        return token.strip()\n","\n","    entities = []\n","    for doc in text:\n","        entity = []\n","        for sent in doc:\n","            data = nlp(sent)\n","            entity.append(data.ents)\n","        entities.append(entity)\n","    return entities\n","\n","\n","\n","def unmask_role_name(unmasker, tokenizer, text):\n","    # Inference role names for each prompt\n","    # Returned 10 candidate roles whose maximum length is 3\n","\n","    DEVICE = unmasker.device\n","    encoded = tokenizer.encode_plus(text, add_special_tokens=True, truncation=True, return_tensors='pt')\n","    input_ids = encoded['input_ids'].to(DEVICE)\n","    outputs = unmasker.generate(input_ids=input_ids,\n","                              num_beams=200, num_return_sequences=10,\n","                              max_length=5)\n","    res = []\n","    end_token='<extra_id_1>'\n","    for output in outputs:\n","        _txt = tokenizer.decode(output[2:], skip_special_tokens=False, clean_up_tokenization_spaces=False)\n","        if end_token in _txt:\n","            _end_token_index = _txt.index(end_token)\n","            res.append(_txt[:_end_token_index])\n","    return res\n","\n","def generate_candidate_role(unmasker, tokenizer, text, entities, event_type, alpha=0.4):\n","    # Generate candidate roles for each named entity according the text\n","\n","    # The geneation output will be saved to dir_path\n","    all_cand = Counter()\n","    dir_path = 'candidate_roles/'\n","    if not os.path.exists(dir_path):\n","        os.makedirs(dir_path)\n","    file_path = dir_path + event_type\n","\n","    if os.path.exists(file_path):\n","        # For the given event type, if there exist the generated roles, just load them for the subsequent steps.\n","        all_cand = torch.load(file_path)\n","    else:\n","        for (doc, entity) in tqdm(zip(text, entities)):\n","            string = ''\n","            cands = []\n","            for i in range(len(doc)):\n","                string += doc[i]\n","                for e in entity[i]:\n","                    # Construct different prompts considering the entity types like CARDINAL, GPE, and PERSON.\n","                    if e.label_ == 'CARDINAL':\n","                        templete = string + ' According to this, the number of <extra_id_0> of this ' + event_type + ' is ' + str(e) + '.'\n","                        # the input of generation model includes the original text and the constructed prompt\n","                        res = unmask_role_name(unmasker, tokenizer, templete)\n","                        cands += res\n","                    elif e.label_ == 'GPE':\n","                        templete = string + ' According to this, the <extra_id_0> is ' + str(e) + ' in this ' + event_type  + '.'\n","                        #templete = string + ' According to this, ' + str(e) + ' is a <extra_id_0> in this ' + event_type  + '.'\n","                        res = unmask_role_name(unmasker, tokenizer, templete)\n","                        cands += res\n","                    elif e.label_ == 'PERSON':\n","                        templete = string + ' According to this, ' + str(e) + ' plays the role of <extra_id_0> in this ' + event_type  + '.'\n","                        res = unmask_role_name(unmasker, tokenizer, templete)\n","                        cands += res\n","                    templete = string + ' According to this, the <extra_id_0> of this ' + event_type + ' is ' + str(e) + '.'\n","                    res = unmask_role_name(unmasker, tokenizer, templete)\n","                    cands += res\n","\n","            cands = set(cands)\n","            cands = Counter(cands)\n","            all_cand += cands\n","\n","        torch.save(all_cand, file_path)\n","\n","    # For each role name, count the number of doucments it can generate from.\n","    # If one role is generated from a few documents, it will be removed.\n","    role_name = [k for k in all_cand if all_cand[k] > alpha*len(text)]\n","    return role_name\n","\n","\n","def extract_argument(question_answerer, text, role_name, event_type, beta=0.3):\n","    # Extract one argument for each candidate role from each document\n","    arguments = []\n","    for doc in text:\n","        full_str = ' '.join(doc)\n","        all_answer = {}\n","        for name in role_name:\n","            # Construct a question\n","            question = 'What is the ' + name + ' of this ' + event_type + '?'\n","            result = question_answerer(question=question, context=full_str)\n","            # Filter out the arguments with low confidence scores.\n","            if result['score'] > beta:\n","                all_answer[name] = (result['answer'], result['score'])\n","        arguments.append(all_answer)\n","    return arguments\n","\n","\n","def select_role_name(role_names, arguments, theta=0.4, lamda=0.4):\n","    # Merge and select salient roles\n","\n","    n_name = len(role_names)\n","    n_file = len(arguments)\n","\n","    # Build a graph to find the roles of similar semantics.\n","    # The nodes are role names.\n","    # If two roles usually have the same argument in each document, they are connected with an edge.\n","    G = nx.Graph()\n","    G.add_nodes_from(role_names)\n","    for i in range(n_name):\n","        for j in range(i):\n","            cnt = 0\n","            for arg in arguments:\n","                if role_names[i] in arg and role_names[j] in arg:\n","                    cnt += arg[role_names[i]][0] == arg[role_names[j]][0]\n","            if cnt > theta * n_file:\n","                G.add_edge(role_names[i], role_names[j])\n","    # The roles in a connected component are similar in semantics, thus being merged together.\n","    cluster = [list(c) for c in nx.connected_components(G)]\n","\n","    # Rank each cluster according the highest frequency of its roles\n","    cluster_score = []\n","    for c in cluster:\n","        score = [role_names.index(i) for i in c]\n","        cluster_score.append(min(score))\n","    ranked_index = np.argsort(cluster_score)\n","    ranked_cluster = [cluster[i] for i in ranked_index]\n","    print(ranked_cluster)\n","\n","    # Each cluster will have one argument at most in one document\n","    # We select the argument according to the confidence score\n","    seleted_args = []\n","    for arg in arguments:\n","        merged_arg = {}\n","        for c in cluster:\n","            score = [arg[i][1] if i in arg else 0.0 for i in c]\n","            max_score = max(score)\n","            max_idx = score.index(max_score)\n","            best_name = c[max_idx]\n","            # Remove the cluster if its highest confidence score is lower than lamda.\n","            if best_name in arg and arg[best_name][1] > lamda:\n","                key = ', '.join(c)\n","                merged_arg[key] = arg[best_name]\n","        seleted_args.append(merged_arg)\n","\n","    # Filter redundent arguments for each document\n","    filter_args = []\n","    for arg in seleted_args:\n","        val_dic = {}\n","        filter_arg = {}\n","        for key in arg:\n","            val, score = arg[key]\n","            if val not in val_dic or val_dic[val][1] < score:\n","                val_dic[val] = (key, score)\n","        for val in val_dic:\n","            key, score = val_dic[val]\n","            filter_arg[key] = (val, score)\n","        filter_args.append(filter_arg)\n","\n","    return ranked_cluster, filter_args\n","\n","\n","\n","\n","def save_output(event_type, schema, arguments, triggers):\n","    # Save the model output as a csv file to OUTPUT_DIR\n","    if not os.path.exists(OUTPUT_DIR):\n","        os.makedirs(OUTPUT_DIR)\n","\n","\n","    path = OUTPUT_DIR + event_type + '.csv'\n","    # The header of the csv file include the roles.\n","    header = [', '.join(name) for name in schema] + ['trigger']\n","    events = []\n","    # In the file, each line has the arguments of one document.\n","    for (i, arg) in enumerate(arguments):\n","        tmp = {'trigger': triggers[i]}\n","        for key in arg:\n","            tmp[key] = arg[key][0]\n","        events.append(tmp)\n","\n","    with open(path, 'w', encoding='UTF8', newline='') as f:\n","        writer = csv.DictWriter(f, fieldnames=header)\n","        writer.writeheader()\n","        writer.writerows(events)\n","\n","\n","\n","def RolePred_pipeline(event_type, DEVICE_ID=0):\n","    path = DATA_DIR + event_type + '/'\n","    print(path)\n","\n","    text, triggers = load_data(path)\n","    text = truncate_text(text)\n","\n","    nlp = spacy.load(\"en_core_web_sm\")\n","    entities = recognize_named_entity(text, nlp)\n","\n","    print(entities)\n","\n","    # Load the T5 model and tokenizer\n","    DEVICE = torch.device('cuda:' + str(DEVICE_ID))\n","    T5_PATH = 't5-base'\n","    t5_tokenizer = T5Tokenizer.from_pretrained(T5_PATH)\n","    t5_config = T5Config.from_pretrained(T5_PATH)\n","    t5_mlm = T5ForConditionalGeneration.from_pretrained(T5_PATH, config=t5_config)\n","\n","    # Move the T5 model to CPU\n","    t5_mlm.to(DEVICE)\n","\n","    role_names = generate_candidate_role(t5_mlm, t5_tokenizer, text, entities, event_type)\n","\n","    # Set up the question answering pipeline for CPU\n","    question_answerer = pipeline('question-answering', model=\"deepset/roberta-large-squad2\", tokenizer=\"deepset/roberta-large-squad2\", device=-1)  # device=-1 for CPU\n","    arguments = extract_argument(question_answerer, text, role_names, event_type)\n","\n","    cluster, merged_arguments = select_role_name(role_names, arguments)\n","    save_output(event_type, cluster, merged_arguments, triggers)\n","\n","DATA_DIR = 'RoleEE_data/'  # dataset directory\n","OUTPUT_DIR = 'output/'  # output directory\n","\n","# Get all subdirectory names from the dataset directory as event types\n","EVENT_TYPE = []\n","for root, dirs, files in os.walk(DATA_DIR, topdown=False):\n","    EVENT_TYPE = dirs\n","    print(EVENT_TYPE)\n","\n","# Inference the argument roles for each event type\n","for event_type in EVENT_TYPE:\n","\n","    RolePred_pipeline(event_type)\n"]},{"cell_type":"code","execution_count":null,"id":"4faea55a","metadata":{"id":"4faea55a"},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.6"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}